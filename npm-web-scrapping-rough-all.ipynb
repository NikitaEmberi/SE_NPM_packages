{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-01-11T05:20:27.051766Z","iopub.status.busy":"2024-01-11T05:20:27.051444Z","iopub.status.idle":"2024-01-11T05:20:32.225033Z","shell.execute_reply":"2024-01-11T05:20:32.223739Z","shell.execute_reply.started":"2024-01-11T05:20:27.051742Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Details for --------------------------------------------------------------------------------------------------------------------------------whynunu written to output_details.csv\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.5\u001b[39m))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[22], line 51\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch details for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# import requests\n","# import csv\n","# import random\n","# from bs4 import BeautifulSoup\n","\n","# def get_package_details(package_name):\n","#     url = f'https://www.npmjs.com/package/{package_name}'\n","#     response = requests.get(url)\n","    \n","#     if response.status_code == 200:\n","#         soup = BeautifulSoup(response.text, 'html.parser')\n","        \n","#         version_element = soup.find('span', class_='_76473bea')\n","#         version = version_element.text.strip() if version_element else 'N/A'\n","        \n","#         package_category_element = soup.find('span', class_='_813b53b2')\n","#         package_category = package_category_element.text.strip() if package_category_element else 'N/A'\n","        \n","#         status_elements = soup.find_all('span', class_='_76473bea')\n","#         status = status_elements[2].next_sibling.strip() if len(status_elements) > 2 and status_elements[2].next_sibling else 'N/A'\n","        \n","#         release_time_element = soup.find('time')\n","#         release_time = release_time_element['title'] if release_time_element else 'N/A'\n","        \n","#         return {'package_name': package_name, 'version': version, 'package_category': package_category, 'status': status, 'release_time': release_time}\n","#     else:\n","#         return None\n","\n","# def main():\n","#     input_csv_path = '/kaggle/input/npm-packages/package-names.csv'\n","#     output_csv_path = 'output_details.csv'\n","\n","#     with open(input_csv_path, 'r') as input_file, open(output_csv_path, 'w', newline='') as output_file:\n","#         reader = csv.reader(input_file)\n","#         next(reader)  # Skip header if present\n","\n","#         fieldnames = ['package_name', 'version', 'package_category', 'status', 'release_time']\n","#         writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n","#         writer.writeheader()\n","\n","#         for row in reader:\n","#             package_name = row[0]\n","#             package_details = get_package_details(package_name)\n","\n","#             if package_details:\n","#                 writer.writerow(package_details)\n","#                 print(f\"Details for {package_name} written to {output_csv_path}\")\n","#             else:\n","#                 print(f\"Failed to fetch details for {package_name}\")\n","                \n","#             time.sleep(random.uniform(0.5, 1.5))\n","\n","# if __name__ == \"__main__\":\n","#     main()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T20:19:11.907078Z","iopub.status.busy":"2024-02-17T20:19:11.906593Z","iopub.status.idle":"2024-02-17T20:19:11.915226Z","shell.execute_reply":"2024-02-17T20:19:11.914007Z","shell.execute_reply.started":"2024-02-17T20:19:11.907040Z"},"trusted":true},"outputs":[],"source":["# import time\n","# import random\n","# import requests\n","# import csv\n","# from bs4 import BeautifulSoup\n","\n","\n","# def get_package_details(package_name):\n","#     url = f'https://www.npmjs.com/package/{package_name}'\n","#     response = requests.get(url)\n","    \n","#     if response.status_code == 200:\n","#         soup = BeautifulSoup(response.text, 'html.parser')\n","        \n","#         version_element = soup.find('span', class_='_76473bea')\n","#         version = version_element.text.strip() if version_element else 'N/A'\n","#         version = version[:-6]\n","        \n","#         package_category_element = soup.find('span', class_='_813b53b2')\n","#         package_category = package_category_element.text.strip() if package_category_element else 'N/A'\n","        \n","#         status_elements = soup.find_all('span', class_='_76473bea')\n","#         status = status_elements[2].next_sibling.strip() if len(status_elements) > 2 and status_elements[2].next_sibling else 'N/A'\n","        \n","#         release_time_element = soup.find('time')\n","#         release_time = release_time_element['title'] if release_time_element else 'N/A'\n","        \n","#         return {'package_name': package_name, 'version': version, 'package_category': package_category, 'status': status, 'release_time': release_time}\n","#     else:\n","#         return None\n","\n","# def main():\n","#     input_csv_path = '/kaggle/input/npm-packages/package-names.csv'\n","#     output_csv_path = 'output_details.csv'\n","\n","#     with open(input_csv_path, 'r') as input_file, open(output_csv_path, 'w', newline='') as output_file:\n","#         reader = csv.reader(input_file)\n","#         next(reader)  # Skip header if present\n","\n","#         fieldnames = ['package_name', 'version', 'package_category', 'status', 'release_time']\n","#         writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n","#         writer.writeheader()\n","\n","#         # Set a counter for the number of requests made\n","#         request_counter = 0\n","\n","#         for row in reader:\n","#             package_name = row[0]\n","#             package_details = get_package_details(package_name)\n","\n","#             if package_details:\n","#                 writer.writerow(package_details)\n","#                 print(f\"Details for {package_name} written to {output_csv_path}\")\n","#             else:\n","#                 print(f\"Failed to fetch details for {package_name}\")\n","\n","#             # Increment the request counter\n","#             request_counter += 1\n","\n","#             # Introduce a random delay after a random number of requests (0 to 5)\n","#             if request_counter % random.randint(1, 2) == 0:\n","#                 delay_time = random.uniform(0.24, 0.56)\n","#                 print(f\"Delaying for {delay_time:.2f} seconds...\")\n","#                 time.sleep(delay_time)\n","\n","# if __name__ == \"__main__\":\n","#     main()\n"]},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-02-17T21:10:54.234038Z","iopub.status.busy":"2024-02-17T21:10:54.233597Z","iopub.status.idle":"2024-02-17T21:18:51.253063Z","shell.execute_reply":"2024-02-17T21:18:51.252158Z","shell.execute_reply.started":"2024-02-17T21:10:54.234001Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['@arthur791004/ns', '@arthur791004/proxy-server', '@arthur791004/redux-lazy', '@arthur_melo/common', '@arthuralmanzor/hello-wasm', '@arthurassuncao/contentful-cms-blog-render', '@arthurcerqueira/core-components', '@arthurcortezz/hybrid-crypto-js', '@arthurdandrea/found', '@arthurdanjou/eslint-config', '@arthurdanjou/eslint-config-basic', '@arthurdanjou/eslint-config-react', '@arthurdanjou/eslint-config-ts', '@arthurdanjou/eslint-config-vue', '@arthurdenner/gramma', '@arthurdenner/use-fetch', '@arthurdevpro/common', '@arthurfiorette/jest-prisma', '@arthurfiorette/jest-prisma-core', '@arthurfiorette/jest-prisma-node', '@arthurfiorette/prettier-config', '@arthurfred/nest-crud', '@arthurgaspar/routinejs', '@arthurgeron/context-selector', '@arthurgeron/eslint-config', '@arthurgeron/eslint-plugin-react-usememo', '@arthurgrigoletto-ui/docs', '@arthurgrigoletto-ui/react', '@arthurgrigoletto-ui/tokens', '@arthurgubaidullin/nx-embed-dependencies', '@arthurhauer/ngx-monaco-editor', '@arthurhovhannisyan31/storybook_test', '@arthurianx/reactblitz', '@arthurk/avalanche', '@arthurka/eslint', '@arthurka/json-serializer', '@arthurka/mongodb', '@arthurka/pipe', '@arthurka/tg-bot', '@arthurka/ts-reset', '@arthurka/ts-utils', '@arthurking/typeorm', '@arthurmj96/printful-js', '@arthurparahyba/html-schema-template', '@arthurparahyba/html-template', '@arthurpedro/linx-component', '@arthurpedro/theme', '@arthurpedroti/eslint-config', '@arthurperret/note-link-janitor-fr', '@arthurpolon/tilt.it', '@arthurrios-ignite-ui/docs', '@arthurrios-ignite-ui/react', '@arthurrios-ignite-ui/tokens', '@arthurrticketing/common', '@arthursegato/simple-parcel-bundler', '@arthurspa/go-watermill-template', '@arthursvpb/eslint-config', '@arthurtimoteo/hacker-chat-client', '@arthurverrept/proto-npm', '@arthuryeti/terra', '@arthuryeti/terraswap', '@arthuryeti/tsconfig', '@arti-art/holidates', '@arti-the-ai/arti-core', '@arti.oga/random-package', '@artibet/react-mui-components', '@artibet/react-sidebar-layout', '@artibox/colors', '@artibox/components', '@artibox/core', '@artibox/editor', '@artibox/icons', '@artibox/locale', '@artibox/slate-blockquote', '@artibox/slate-bold', '@artibox/slate-common', '@artibox/slate-editor', '@artibox/slate-facebook', '@artibox/slate-file-uploader', '@artibox/slate-heading', '@artibox/slate-highlight', '@artibox/slate-image', '@artibox/slate-input-block', '@artibox/slate-instagram', '@artibox/slate-italic', '@artibox/slate-jsx-serializer', '@artibox/slate-link', '@artibox/slate-list', '@artibox/slate-react', '@artibox/slate-separation-line', '@artibox/slate-soft-break', '@artibox/slate-strikethrough', '@artibox/slate-toggle-mark', '@artibox/slate-toolbar', '@artibox/slate-underline', '@artibox/slate-video', '@artibox/theme', '@artibox/theme-dark', '@artibox/utils', '@artichain_finance/artichain-eslint-config', '@artichain_finance/artichain-swap-uikit', '@articho28-tickets/common', '@artichokeruby/logo', '@artick/common', '@articket/common', '@artickets/common', '@articketsprojects/common', '@artickit/common', '@articks/common', '@articl.net/articl-spa', '@articles-searcher/api-service', '@articles-searcher/crossref-model', '@articles-searcher/risc-model', '@articles-searcher/scopus-model', '@articles-searcher/wos-model', '@articstudio/js-bin', '@articulate/asyncios', '@articulate/authentic', '@articulate/chromeless', '@articulate/consul-sync', '@articulate/ducks', '@articulate/dynapro', '@articulate/eslint-config-rise', '@articulate/eslint-plugin-global', '@articulate/funky', '@articulate/gimme', '@articulate/hapi-api-versions', '@articulate/hapi-authentic', '@articulate/hapi-json-view', '@articulate/hapi-logging', '@articulate/hermes', '@articulate/hubot-pager-me', '@articulate/invoke', '@articulate/koala', '@articulate/kongfig', '@articulate/node-saml', '@articulate/node-unzipper', '@articulate/orson', '@articulate/paperplane-bugsnag', '@articulate/progress', '@articulate/react-animate-on-scroll', '@articulate/rise-mp3-recorder', '@articulate/sox', '@articulate/sox-newrelic', '@articulate/spy', '@articulate/squiss-jobs', '@articulate/tinygen', '@articulate/xliff', '@articulatesydney/gmservice', '@articum/mta-types-cef', '@artid/azwallet', '@artid/azwallet-kit', '@artid/azwallet-loader', '@artidata/hello-npm', '@artie-owlet/amqp-routing-match', '@artie-owlet/amqplib-wrapper', '@artie-owlet/cabbit', '@artie-owlet/clicker-calc', '@artie-owlet/intransitive-dice', '@artie_f/declare-js', '@artiefuzzz/arch', '@artiefuzzz/eslint-config', '@artiefuzzz/lynx', '@artiefuzzz/north', '@artiefuzzz/sakura', '@artiefuzzz/ts', '@artiefuzzz/ttl', '@artiefuzzz/utils', '@artieworld/geofirex', '@artifact-project/css', '@artifact-project/easystate', '@artifact-project/i18n', '@artifact-project/morph', '@artifact-project/recaptcha', '@artifact-project/webauthn', '@artifacter/cli', '@artifacter/common', '@artifacter/core', '@artifacter/template-engine', '@artifacter/webapi', '@artifacter/worker', '@artifactlabs/open-graph', '@artifactlabsofficial/refinable-sdk', '@artifactsio/sl-vue3-tree', '@artifak/block', '@artifak/bundler', '@artifak/component-generator', '@artifak/container', '@artifak/flex', '@artifak/fluidsizing', '@artifak/grid', '@artifak/hextorgb', '@artifak/hextorgba', '@artifak/image', '@artifak/imagery', '@artifak/media', '@artifak/pxtoem', '@artifak/pxtorem', '@artifak/text-input', '@artifak/typography']\n","Error fetching data for @arthur_melo/common: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthur_melo/common\n","Error fetching data for @arthurfred/nest-crud: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthurfred/nest-crud\n","Error fetching data for @arthursegato/simple-parcel-bundler: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthursegato/simple-parcel-bundler\n","Error fetching data for @articulate/authentic: 404 Client Error: Not Found for url: https://registry.npmjs.org/@articulate/authentic\n","Error fetching data for @articulate/orson: 404 Client Error: Not Found for url: https://registry.npmjs.org/@articulate/orson\n"]}],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","# #         package_names.append(row[0].lstrip('@'))  # Remove leading '@'\n","\n","# package_names = package_names[70000:70200]\n","\n","# print(package_names)\n","\n","# # Create an empty DataFrame to store the retrieved data\n","# data = pd.DataFrame(columns=['package_name', 'metadata', 'download_stats'])\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","\n","#     try:\n","#         response = requests.get(url)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","#         package_data = response.json()\n","\n","#         # Get download stats separately\n","#         try:\n","#             download_stats = requests.get(f'https://registry.npmjs.org/{package_name}/downloads').json()\n","#         except requests.exceptions.RequestException:\n","#             download_stats = None  # Indicate download stats unavailable\n","\n","#         # Create a DataFrame with a single row and index 0\n","#         data_row = pd.DataFrame({\n","#             'package_name': [package_name],  # wrap package_name in a list to ensure it's not a scalar value\n","#             'metadata': [package_data],      # wrap package_data in a list to ensure it's not a scalar value\n","#             'download_stats': [download_stats]  # wrap download_stats in a list to ensure it's not a scalar value\n","#         })  # Do not specify index since it's only one row\n","\n","#         # Concatenate the row to the main DataFrame\n","#         data = pd.concat([data, data_row], ignore_index=True)\n","        \n","#     except requests.exceptions.RequestException as e:\n","#         # Create a row with \"data not available\" for missing data\n","#         data_row = pd.DataFrame({\n","#             'package_name': [package_name],  # wrap package_name in a list to ensure it's not a scalar value\n","#             'metadata': [None],               # wrap None in a list to ensure it's not a scalar value\n","#             'download_stats': [None]         # wrap None in a list to ensure it's not a scalar value\n","#         })  # Do not specify index since it's only one row\n","        \n","#         # Concatenate the row to the main DataFrame\n","#         data = pd.concat([data, data_row], ignore_index=True)\n","        \n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data.csv', index=False)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-02-17T21:25:37.789681Z","iopub.status.busy":"2024-02-17T21:25:37.789267Z","iopub.status.idle":"2024-02-17T21:31:41.793994Z","shell.execute_reply":"2024-02-17T21:31:41.792978Z","shell.execute_reply.started":"2024-02-17T21:25:37.789644Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Error fetching data for @arthur_melo/common: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthur_melo/common\n","Error fetching data for @arthurfred/nest-crud: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthurfred/nest-crud\n","Error fetching data for @arthursegato/simple-parcel-bundler: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthursegato/simple-parcel-bundler\n","Error fetching data for @articulate/authentic: 404 Client Error: Not Found for url: https://registry.npmjs.org/@articulate/authentic\n","Error fetching data for @articulate/orson: 404 Client Error: Not Found for url: https://registry.npmjs.org/@articulate/orson\n"]}],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","\n","# package_names = package_names[70000:70200]\n","\n","# # print(package_names)\n","\n","# # Create an empty list to store the retrieved data\n","# data_rows = []\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","\n","#     try:\n","#         response = requests.get(url)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","#         package_data = response.json()\n","\n","#         # Extracting required fields from metadata\n","#         description = package_data.get('description', None)\n","#         maintainers_length = len(package_data.get('maintainers', []))\n","#         readmeFilename_exists = 'readmeFilename' in package_data\n","#         created = package_data.get('time', {}).get('created', None)\n","#         modified = package_data.get('time', {}).get('modified', None)\n","#         last_version = list(package_data.get('versions', {}).values())[-1] if package_data.get('versions') else None\n","\n","#         # Create a row for the DataFrame\n","#         data_row = {\n","#             'package_name': package_name,\n","#             'description': description,\n","#             'maintainers_length': maintainers_length,\n","#             'readmeFilename_exists': readmeFilename_exists,\n","#             'created': created,\n","#             'modified': modified,\n","#             'last_version': last_version\n","#         }\n","\n","#         # Append the row to the list\n","#         data_rows.append(data_row)\n","        \n","#     except requests.exceptions.RequestException as e:\n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Create a DataFrame from the list of rows\n","# data = pd.DataFrame(data_rows)\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data.csv', index=False)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-02-17T21:42:17.446029Z","iopub.status.busy":"2024-02-17T21:42:17.445606Z","iopub.status.idle":"2024-02-17T21:43:30.469154Z","shell.execute_reply":"2024-02-17T21:43:30.467965Z","shell.execute_reply.started":"2024-02-17T21:42:17.445994Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['@arthur791004/ns', '@arthur791004/proxy-server', '@arthur791004/redux-lazy', '@arthur_melo/common', '@arthuralmanzor/hello-wasm', '@arthurassuncao/contentful-cms-blog-render', '@arthurcerqueira/core-components', '@arthurcortezz/hybrid-crypto-js', '@arthurdandrea/found', '@arthurdanjou/eslint-config', '@arthurdanjou/eslint-config-basic', '@arthurdanjou/eslint-config-react', '@arthurdanjou/eslint-config-ts', '@arthurdanjou/eslint-config-vue', '@arthurdenner/gramma', '@arthurdenner/use-fetch', '@arthurdevpro/common', '@arthurfiorette/jest-prisma', '@arthurfiorette/jest-prisma-core', '@arthurfiorette/jest-prisma-node', '@arthurfiorette/prettier-config', '@arthurfred/nest-crud', '@arthurgaspar/routinejs', '@arthurgeron/context-selector', '@arthurgeron/eslint-config', '@arthurgeron/eslint-plugin-react-usememo', '@arthurgrigoletto-ui/docs', '@arthurgrigoletto-ui/react', '@arthurgrigoletto-ui/tokens', '@arthurgubaidullin/nx-embed-dependencies', '@arthurhauer/ngx-monaco-editor', '@arthurhovhannisyan31/storybook_test', '@arthurianx/reactblitz', '@arthurk/avalanche', '@arthurka/eslint', '@arthurka/json-serializer', '@arthurka/mongodb', '@arthurka/pipe', '@arthurka/tg-bot', '@arthurka/ts-reset']\n","Error fetching data for @arthur_melo/common: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthur_melo/common\n","Error fetching data for @arthurfred/nest-crud: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthurfred/nest-crud\n"]}],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","# from urllib.parse import urlparse\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","\n","# package_names = package_names[70000:70040]\n","\n","# print(package_names)\n","\n","# # Create an empty list to store the retrieved data\n","# data_rows = []\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","\n","#     try:\n","#         response = requests.get(url)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","#         package_data = response.json()\n","\n","#         # Extracting required fields from metadata\n","#         description = package_data.get('description', None)\n","#         maintainers_length = len(package_data.get('maintainers', []))\n","#         readmeFilename_exists = 'readmeFilename' in package_data\n","#         created = package_data.get('time', {}).get('created', None)\n","#         modified = package_data.get('time', {}).get('modified', None)\n","#         last_version_data = list(package_data.get('versions', {}).values())[-1] if package_data.get('versions') else None\n","\n","#         # Extracting version and cleaning git URL from last version data\n","#         latest_version = last_version_data.get('version', None)\n","#         repository_url = last_version_data.get('repository', {}).get('url', None)\n","#         if repository_url:\n","#             # Clean the git URL\n","#             parsed_url = urlparse(repository_url)\n","#             cleaned_repository_url = parsed_url.scheme + '://' + parsed_url.netloc + parsed_url.path.rstrip('.git')\n","#         else:\n","#             cleaned_repository_url = None\n","\n","#         # Create a row for the DataFrame\n","#         data_row = {\n","#             'package_name': package_name,\n","#             'description': description,\n","#             'maintainers_length': maintainers_length,\n","#             'readmeFilename_exists': readmeFilename_exists,\n","#             'created': created,\n","#             'modified': modified,\n","#             'latest_version': latest_version,\n","#             'latest_version_git_repo': cleaned_repository_url\n","#         }\n","\n","#         # Append the row to the list\n","#         data_rows.append(data_row)\n","        \n","#     except requests.exceptions.RequestException as e:\n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Create a DataFrame from the list of rows\n","# data = pd.DataFrame(data_rows)\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Main Part-1"]},{"cell_type":"code","execution_count":30,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-02-17T22:18:17.945418Z","iopub.status.busy":"2024-02-17T22:18:17.944973Z","iopub.status.idle":"2024-02-17T22:19:31.270458Z","shell.execute_reply":"2024-02-17T22:19:31.269345Z","shell.execute_reply.started":"2024-02-17T22:18:17.945382Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['@arthur791004/ns', '@arthur791004/proxy-server', '@arthur791004/redux-lazy', '@arthur_melo/common', '@arthuralmanzor/hello-wasm', '@arthurassuncao/contentful-cms-blog-render', '@arthurcerqueira/core-components', '@arthurcortezz/hybrid-crypto-js', '@arthurdandrea/found', '@arthurdanjou/eslint-config', '@arthurdanjou/eslint-config-basic', '@arthurdanjou/eslint-config-react', '@arthurdanjou/eslint-config-ts', '@arthurdanjou/eslint-config-vue', '@arthurdenner/gramma', '@arthurdenner/use-fetch', '@arthurdevpro/common', '@arthurfiorette/jest-prisma', '@arthurfiorette/jest-prisma-core', '@arthurfiorette/jest-prisma-node', '@arthurfiorette/prettier-config', '@arthurfred/nest-crud', '@arthurgaspar/routinejs', '@arthurgeron/context-selector', '@arthurgeron/eslint-config', '@arthurgeron/eslint-plugin-react-usememo', '@arthurgrigoletto-ui/docs', '@arthurgrigoletto-ui/react', '@arthurgrigoletto-ui/tokens', '@arthurgubaidullin/nx-embed-dependencies', '@arthurhauer/ngx-monaco-editor', '@arthurhovhannisyan31/storybook_test', '@arthurianx/reactblitz', '@arthurk/avalanche', '@arthurka/eslint', '@arthurka/json-serializer', '@arthurka/mongodb', '@arthurka/pipe', '@arthurka/tg-bot', '@arthurka/ts-reset']\n","Error fetching data for @arthur_melo/common: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthur_melo/common\n","Error fetching data for @arthurfred/nest-crud: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthurfred/nest-crud\n"]}],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","# from urllib.parse import urlparse\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","\n","# package_names = package_names[70000:70040]\n","\n","# print(package_names)\n","\n","# # Create an empty list to store the retrieved data\n","# data_rows = []\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","\n","#     try:\n","#         response = requests.get(url)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","#         package_data = response.json()\n","\n","#         # Extracting required fields from metadata\n","#         description = package_data.get('description', None)\n","#         maintainers_length = len(package_data.get('maintainers', []))\n","#         readmeFilename_exists = 'readmeFilename' in package_data\n","#         created = package_data.get('time', {}).get('created', None)\n","#         modified = package_data.get('time', {}).get('modified', None)\n","#         last_version_data = list(package_data.get('versions', {}).values())[-1] if package_data.get('versions') else None\n","\n","#         # Extracting version and git URL from last version data\n","#         latest_version = last_version_data.get('version', None)\n","#         repository_url = last_version_data.get('repository', {}).get('url', None)\n","\n","#         # Clean the git URL\n","#         if repository_url:\n","#             parsed_url = urlparse(repository_url)\n","#             if parsed_url.scheme == 'git':\n","#                 cleaned_repository_url = parsed_url.netloc + parsed_url.path\n","#             else:\n","#                 cleaned_repository_url = repository_url.lstrip('git+')\n","\n","#         # Create a row for the DataFrame\n","#         data_row = {\n","#             'package_name': package_name,\n","#             'description': description,\n","#             'maintainers_length': maintainers_length,\n","#             'readmeFilename_exists': readmeFilename_exists,\n","#             'created': created,\n","#             'modified': modified,\n","#             'latest_version': latest_version,\n","#             'latest_version_git_repo': cleaned_repository_url\n","#         }\n","\n","#         # Append the row to the list\n","#         data_rows.append(data_row)\n","\n","#     except requests.exceptions.RequestException as e:\n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Create a DataFrame from the list of rows\n","# data = pd.DataFrame(data_rows)\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Main Part-2"]},{"cell_type":"code","execution_count":29,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-02-17T22:15:28.373106Z","iopub.status.busy":"2024-02-17T22:15:28.372656Z","iopub.status.idle":"2024-02-17T22:16:43.560384Z","shell.execute_reply":"2024-02-17T22:16:43.559198Z","shell.execute_reply.started":"2024-02-17T22:15:28.373069Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['@arthur791004/ns', '@arthur791004/proxy-server', '@arthur791004/redux-lazy', '@arthur_melo/common', '@arthuralmanzor/hello-wasm', '@arthurassuncao/contentful-cms-blog-render', '@arthurcerqueira/core-components', '@arthurcortezz/hybrid-crypto-js', '@arthurdandrea/found', '@arthurdanjou/eslint-config', '@arthurdanjou/eslint-config-basic', '@arthurdanjou/eslint-config-react', '@arthurdanjou/eslint-config-ts', '@arthurdanjou/eslint-config-vue', '@arthurdenner/gramma', '@arthurdenner/use-fetch', '@arthurdevpro/common', '@arthurfiorette/jest-prisma', '@arthurfiorette/jest-prisma-core', '@arthurfiorette/jest-prisma-node', '@arthurfiorette/prettier-config', '@arthurfred/nest-crud', '@arthurgaspar/routinejs', '@arthurgeron/context-selector', '@arthurgeron/eslint-config', '@arthurgeron/eslint-plugin-react-usememo', '@arthurgrigoletto-ui/docs', '@arthurgrigoletto-ui/react', '@arthurgrigoletto-ui/tokens', '@arthurgubaidullin/nx-embed-dependencies', '@arthurhauer/ngx-monaco-editor', '@arthurhovhannisyan31/storybook_test', '@arthurianx/reactblitz', '@arthurk/avalanche', '@arthurka/eslint', '@arthurka/json-serializer', '@arthurka/mongodb', '@arthurka/pipe', '@arthurka/tg-bot', '@arthurka/ts-reset']\n","Error fetching data for @arthur_melo/common: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthur_melo/common\n","Error fetching data for @arthurfred/nest-crud: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthurfred/nest-crud\n"]}],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","# from urllib.parse import urlparse\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","\n","# package_names = package_names[70000:70040]\n","\n","# print(package_names)\n","\n","# # Create an empty list to store the retrieved data\n","# data_rows = []\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","#     headers = {'Accept': 'application/vnd.npm.install-v1+json'}\n","\n","#     try:\n","#         # Fetching the abbreviated response\n","#         response = requests.get(url, headers=headers)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","#         abbreviated_package_data = response.json()\n","\n","#         # Extracting required fields from metadata\n","#         description = abbreviated_package_data.get('description', None)\n","#         maintainers_length = len(abbreviated_package_data.get('maintainers', []))\n","#         readmeFilename_exists = 'readmeFilename' in abbreviated_package_data\n","#         created = abbreviated_package_data.get('time', {}).get('created', None)\n","#         modified = abbreviated_package_data.get('time', {}).get('modified', None)\n","#         last_version_data = list(abbreviated_package_data.get('versions', {}).values())[-1] if abbreviated_package_data.get('versions') else None\n","#         latest_version = last_version_data.get('version', None)\n","\n","#         # Extract dependencies, devDependencies, and deprecated\n","#         dependencies = last_version_data.get('dependencies', {})\n","#         dependencies_name = list(dependencies.keys())\n","#         dependencies_count = len(dependencies)\n","\n","#         dev_dependencies = last_version_data.get('devDependencies', {})\n","#         dev_dependencies_name = list(dev_dependencies.keys())\n","#         dev_dependencies_count = len(dev_dependencies)\n","\n","#         deprecated = last_version_data.get('deprecated', None)\n","#         deprecated_status = 'Yes' if deprecated else 'No'\n","\n","#         # Cleaning the git URL\n","#         repository_url = last_version_data.get('repository', {}).get('url', None)\n","#         if repository_url:\n","#             parsed_url = urlparse(repository_url)\n","#             if parsed_url.scheme == 'git':\n","#                 cleaned_repository_url = parsed_url.netloc + parsed_url.path\n","#             else:\n","#                 cleaned_repository_url = repository_url.lstrip('git+')\n","\n","#         # Create a row for the DataFrame\n","#         data_row = {\n","#             'package_name': package_name,\n","#             'description': description,\n","#             'maintainers_length': maintainers_length,\n","#             'readmeFilename_exists': readmeFilename_exists,\n","#             'created': created,\n","#             'modified': modified,\n","#             'latest_version': latest_version,\n","#             'latest_version_git_repo': cleaned_repository_url,\n","#             'dependencies_name': dependencies_name,\n","#             'dependencies_count': dependencies_count,\n","#             'dev_dependencies_name': dev_dependencies_name,\n","#             'dev_dependencies_count': dev_dependencies_count,\n","#             'deprecated': deprecated_status,\n","#         }\n","\n","#         # Append the row to the list\n","#         data_rows.append(data_row)\n","\n","#     except requests.exceptions.RequestException as e:\n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Create a DataFrame from the list of rows\n","# data = pd.DataFrame(data_rows)\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Main-Combined"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T22:28:44.192904Z","iopub.status.busy":"2024-02-17T22:28:44.192433Z","iopub.status.idle":"2024-02-17T22:30:55.483651Z","shell.execute_reply":"2024-02-17T22:30:55.482392Z","shell.execute_reply.started":"2024-02-17T22:28:44.192865Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Error fetching data for @arthur_melo/common: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthur_melo/common\n","Error fetching data for @arthurfred/nest-crud: 404 Client Error: Not Found for url: https://registry.npmjs.org/@arthurfred/nest-crud\n"]}],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","# from urllib.parse import urlparse\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","\n","# package_names = package_names[70000:70040]\n","\n","# # Create an empty list to store the retrieved data\n","# data_rows = []\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","#     headers = {'Accept': 'application/vnd.npm.install-v1+json'}\n","\n","#     try:\n","#         # Fetching the full metadata response\n","#         response = requests.get(url)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","#         package_data = response.json()\n","\n","#         # Extracting required fields from metadata\n","#         description = package_data.get('description', None)\n","#         maintainers_length = len(package_data.get('maintainers', []))\n","#         readmeFilename_exists = 'readmeFilename' in package_data\n","#         created = package_data.get('time', {}).get('created', None)\n","#         modified = package_data.get('time', {}).get('modified', None)\n","#         last_version_data = list(package_data.get('versions', {}).values())[-1] if package_data.get('versions') else None\n","\n","#         # Extracting version and git URL from last version data\n","#         latest_version = last_version_data.get('version', None)\n","#         repository_url = last_version_data.get('repository', {}).get('url', None)\n","\n","#         # Clean the git URL\n","#         if repository_url:\n","#             parsed_url = urlparse(repository_url)\n","#             if parsed_url.scheme == 'git':\n","#                 cleaned_repository_url = parsed_url.netloc + parsed_url.path\n","#             else:\n","#                 cleaned_repository_url = repository_url.lstrip('git+')\n","\n","#         # Fetching the abbreviated response\n","#         response = requests.get(url, headers=headers)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","#         abbreviated_package_data = response.json()\n","\n","#         # Extracting dependencies, devDependencies, and deprecated from abbreviated response\n","#         last_version_data = list(abbreviated_package_data.get('versions', {}).values())[-1] if abbreviated_package_data.get('versions') else None\n","#         dependencies = last_version_data.get('dependencies', {})\n","#         dependencies_name = list(dependencies.keys())\n","#         dependencies_count = len(dependencies)\n","\n","#         dev_dependencies = last_version_data.get('devDependencies', {})\n","#         dev_dependencies_name = list(dev_dependencies.keys())\n","#         dev_dependencies_count = len(dev_dependencies)\n","\n","#         deprecated = last_version_data.get('deprecated', None)\n","#         deprecated_status = 'Yes' if deprecated else 'No'\n","\n","#         # Create a row for the DataFrame\n","#         data_row = {\n","#             'package_name': package_name,\n","#             'description': description,\n","#             'maintainers_length': maintainers_length,\n","#             'readmeFilename_exists': readmeFilename_exists,\n","#             'created': created,\n","#             'modified': modified,\n","#             'latest_version': latest_version,\n","#             'latest_version_git_repo': cleaned_repository_url,\n","#             'dependencies_name': dependencies_name,\n","#             'dependencies_count': dependencies_count,\n","#             'dev_dependencies_name': dev_dependencies_name,\n","#             'dev_dependencies_count': dev_dependencies_count,\n","#             'deprecated': deprecated_status,\n","#         }\n","\n","#         # Append the row to the list\n","#         data_rows.append(data_row)\n","\n","#     except requests.exceptions.RequestException as e:\n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Create a DataFrame from the list of rows\n","# data = pd.DataFrame(data_rows)\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data_combined.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Main-Downloads"]},{"cell_type":"code","execution_count":40,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-02-17T22:53:39.610589Z","iopub.status.busy":"2024-02-17T22:53:39.610169Z","iopub.status.idle":"2024-02-17T22:54:03.179364Z","shell.execute_reply":"2024-02-17T22:54:03.177737Z","shell.execute_reply.started":"2024-02-17T22:53:39.610552Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"ename":"AttributeError","evalue":"'str' object has no attribute 'get'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Extracting version and git URL from last version data\u001b[39;00m\n\u001b[1;32m     45\u001b[0m latest_version \u001b[38;5;241m=\u001b[39m last_version_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m repository_url \u001b[38;5;241m=\u001b[39m \u001b[43mlast_version_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrepository\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Clean the git URL\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repository_url:\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"]}],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","# from urllib.parse import urlparse\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","\n","# package_names = package_names[90000:91000]\n","\n","# # Create an empty list to store the retrieved data\n","# data_rows = []\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","#     headers = {'Accept': 'application/vnd.npm.install-v1+json'}\n","\n","#     try:\n","#         # Fetching the full metadata response\n","#         response = requests.get(url)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","#         package_data = response.json()\n","\n","#         # Extracting required fields from metadata\n","#         description = package_data.get('description', None)\n","#         maintainers_length = len(package_data.get('maintainers', []))\n","#         readmeFilename_exists = 'readmeFilename' in package_data\n","#         created = package_data.get('time', {}).get('created', None)\n","#         modified = package_data.get('time', {}).get('modified', None)\n","#         last_version_data = list(package_data.get('versions', {}).values())[-1] if package_data.get('versions') else None\n","\n","#         # Extracting version and git URL from last version data\n","#         latest_version = last_version_data.get('version', None)\n","#         repository_url = last_version_data.get('repository', {}).get('url', None)\n","\n","#         # Clean the git URL\n","#         if repository_url:\n","#             parsed_url = urlparse(repository_url)\n","#             if parsed_url.scheme == 'git':\n","#                 cleaned_repository_url = parsed_url.netloc + parsed_url.path\n","#             else:\n","#                 cleaned_repository_url = repository_url.lstrip('git+')\n","\n","#         # Fetching the abbreviated response\n","#         response = requests.get(url, headers=headers)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","#         abbreviated_package_data = response.json()\n","\n","#         # Extracting dependencies, devDependencies, and deprecated from abbreviated response\n","#         last_version_data = list(abbreviated_package_data.get('versions', {}).values())[-1] if abbreviated_package_data.get('versions') else None\n","#         dependencies = last_version_data.get('dependencies', {})\n","#         dependencies_name = list(dependencies.keys())\n","#         dependencies_count = len(dependencies)\n","\n","#         dev_dependencies = last_version_data.get('devDependencies', {})\n","#         dev_dependencies_name = list(dev_dependencies.keys())\n","#         dev_dependencies_count = len(dev_dependencies)\n","\n","#         deprecated = last_version_data.get('deprecated', None)\n","#         deprecated_status = 'Yes' if deprecated else 'No'\n","\n","#         # Fetching download stats\n","#         download_stats_last_day = requests.get(f'https://api.npmjs.org/downloads/point/last-day/{package_name}').json()\n","#         download_stats_last_week = requests.get(f'https://api.npmjs.org/downloads/range/last-week/{package_name}').json()\n","#         download_stats_last_month = requests.get(f'https://api.npmjs.org/downloads/range/last-month/{package_name}').json()\n","\n","#         # Extracting download stats\n","#         total_downloads = download_stats_last_day['downloads']\n","#         last_week_downloads = sum(day['downloads'] for day in download_stats_last_week['downloads'])\n","#         last_month_downloads = sum(day['downloads'] for day in download_stats_last_month['downloads'])\n","\n","#         # Create a row for the DataFrame\n","#         data_row = {\n","#             'package_name': package_name,\n","#             'description': description,\n","#             'maintainers_length': maintainers_length,\n","#             'readmeFilename_exists': readmeFilename_exists,\n","#             'created': created,\n","#             'modified': modified,\n","#             'latest_version': latest_version,\n","#             'latest_version_git_repo': cleaned_repository_url,\n","#             'dependencies_name': dependencies_name,\n","#             'dependencies_count': dependencies_count,\n","#             'dev_dependencies_name': dev_dependencies_name,\n","#             'dev_dependencies_count': dev_dependencies_count,\n","#             'deprecated': deprecated_status,\n","#             'total_downloads': total_downloads,\n","#             'last_week_downloads': last_week_downloads,\n","#             'last_month_downloads': last_month_downloads\n","#         }\n","\n","#         # Append the row to the list\n","#         data_rows.append(data_row)\n","\n","#     except requests.exceptions.RequestException as e:\n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Create a DataFrame from the list of rows\n","# data = pd.DataFrame(data_rows)\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data_combined.csv', index=False)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-02-17T22:51:33.034409Z","iopub.status.busy":"2024-02-17T22:51:33.033985Z","iopub.status.idle":"2024-02-17T22:51:33.043961Z","shell.execute_reply":"2024-02-17T22:51:33.042771Z","shell.execute_reply.started":"2024-02-17T22:51:33.034373Z"},"trusted":true},"outputs":[],"source":["# import pandas as pd\n","# import requests\n","# import csv\n","# from urllib.parse import urlparse\n","\n","# # Open the CSV file in read mode\n","# with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","#     reader = csv.reader(csvfile)\n","\n","#     # Handle potential header row\n","#     is_header = True\n","#     package_names = []\n","#     for row in reader:\n","#         if is_header:\n","#             is_header = False  # Skip the header row if present\n","#             continue\n","#         package_names.append(row[0])  # Assuming package names are in the first column\n","\n","# package_names = package_names[90000:90040]\n","\n","# # Create an empty list to store the retrieved data\n","# data_rows = []\n","\n","# # Iterate through each package name and fetch data\n","# for package_name in package_names:\n","#     url = f'https://registry.npmjs.org/{package_name}'\n","#     headers = {'Accept': 'application/vnd.npm.install-v1+json'}\n","\n","#     try:\n","#         # Fetching the full metadata response\n","#         response = requests.get(url)\n","#         response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","#         package_data = response.json()\n","\n","#         # Extracting required fields from metadata\n","#         description = package_data.get('description', None)\n","#         maintainers_length = len(package_data.get('maintainers', []))\n","#         readmeFilename_exists = 'readmeFilename' in package_data\n","#         created = package_data.get('time', {}).get('created', None)\n","#         modified = package_data.get('time', {}).get('modified', None)\n","#         last_version_data = list(package_data.get('versions', {}).values())[-1] if package_data.get('versions') else None\n","\n","#         # Extracting version from last version data\n","#         latest_version = last_version_data.get('version', None)\n","        \n","#         # Extracting repository URL if available\n","#         repository_url = None\n","#         if last_version_data:\n","#             repository_info = last_version_data.get('repository')\n","#             if repository_info and isinstance(repository_info, dict):\n","#                 repository_url = repository_info.get('url')\n","\n","#         # Clean the git URL\n","#         if repository_url:\n","#             parsed_url = urlparse(repository_url)\n","#             if parsed_url.scheme == 'git':\n","#                 cleaned_repository_url = parsed_url.netloc + parsed_url.path\n","#             else:\n","#                 cleaned_repository_url = repository_url.lstrip('git+')\n","\n","#         # Fetching download stats\n","#         download_stats_last_day = requests.get(f'https://api.npmjs.org/downloads/point/last-day/{package_name}').json()\n","#         download_stats_last_week = requests.get(f'https://api.npmjs.org/downloads/range/last-week/{package_name}').json()\n","#         download_stats_last_month = requests.get(f'https://api.npmjs.org/downloads/range/last-month/{package_name}').json()\n","\n","#         # Extracting download stats\n","#         total_downloads = download_stats_last_day['downloads']\n","#         last_week_downloads = sum(day['downloads'] for day in download_stats_last_week['downloads'])\n","#         last_month_downloads = sum(day['downloads'] for day in download_stats_last_month['downloads'])\n","\n","#         # Create a row for the DataFrame\n","#         data_row = {\n","#             'package_name': package_name,\n","#             'description': description,\n","#             'maintainers_length': maintainers_length,\n","#             'readmeFilename_exists': readmeFilename_exists,\n","#             'created': created,\n","#             'modified': modified,\n","#             'latest_version': latest_version,\n","#             'latest_version_git_repo': cleaned_repository_url,\n","#             'total_downloads': total_downloads,\n","#             'last_week_downloads': last_week_downloads,\n","#             'last_month_downloads': last_month_downloads\n","#         }\n","\n","#         # Append the row to the list\n","#         data_rows.append(data_row)\n","\n","#     except requests.exceptions.RequestException as e:\n","#         print(f\"Error fetching data for {package_name}: {e}\")\n","\n","# # Create a DataFrame from the list of rows\n","# data = pd.DataFrame(data_rows)\n","\n","# # Save the DataFrame to a new CSV file\n","# data.to_csv('npm_package_data_with_downloads.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Full-Working"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T04:51:49.165623Z","iopub.status.busy":"2024-02-19T04:51:49.165250Z","iopub.status.idle":"2024-02-19T04:53:47.022664Z","shell.execute_reply":"2024-02-19T04:53:47.021004Z","shell.execute_reply.started":"2024-02-19T04:51:49.165592Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import requests\n","import csv\n","from urllib.parse import urlparse\n","\n","# Open the CSV file in read mode\n","with open('/kaggle/input/npm-packages/package-names.csv', 'r') as csvfile:\n","    reader = csv.reader(csvfile)\n","\n","    # Handle potential header row\n","    is_header = True\n","    package_names = []\n","    for row in reader:\n","        if is_header:\n","            is_header = False  # Skip the header row if present\n","            continue\n","        package_names.append(row[0])  # Assuming package names are in the first column\n","\n","package_names = package_names[90100:90200]\n","\n","# Create an empty list to store the retrieved data\n","data_rows = []\n","\n","# Iterate through each package name and fetch data\n","for package_name in package_names:\n","    try:\n","        url = f'https://registry.npmjs.org/{package_name}'\n","        headers = {'Accept': 'application/vnd.npm.install-v1+json'}\n","\n","        # Fetching the full metadata response\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an exception for non-200 status codes\n","\n","        package_data = response.json()\n","\n","        # Extracting required fields from metadata\n","        description = package_data.get('description', None)\n","        maintainers_length = len(package_data.get('maintainers', []))\n","        readmeFilename_exists = 'readmeFilename' in package_data\n","        created = package_data.get('time', {}).get('created', None)\n","        modified = package_data.get('time', {}).get('modified', None)\n","        last_version_data = list(package_data.get('versions', {}).values())[-1] if package_data.get('versions') else None\n","        \n","        # Extracting version and git URL from last version data if available\n","        latest_version = None\n","        repository_url = None\n","        cleaned_repository_url = None\n","        if isinstance(last_version_data, dict):\n","            latest_version = last_version_data.get('version', None)\n","            # Check if 'repository' exists and is a dictionary\n","            repository_data = last_version_data.get('repository')\n","            if isinstance(repository_data, dict):\n","                repository_url = repository_data.get('url', None)\n","                # Convert SSH URL to HTTPS for GitHub repositories\n","                if repository_url and 'github.com' in repository_url and (repository_url.startswith('git+') or repository_url.startswith('ssh://git@github.com') or repository_url.startswith('https://github.com') or repository_url.startswith('git://github.com')):\n","                    repository_url = repository_url.replace('git+ssh://git@github.com', 'https://github.com')\n","                    repository_url = repository_url.replace('ssh://git@github.com', 'https://github.com')\n","                    repository_url = repository_url.replace('git://github.com', 'https://github.com')\n","                    \n","                    repository_url = repository_url[:-4] if repository_url.endswith('.git') else repository_url\n","                else:\n","                    print(f'skipped: {repository_url}')\n","                # Clean the git URL\n","                if repository_url:\n","                    parsed_url = urlparse(repository_url)\n","                    if parsed_url.scheme == 'git':\n","                        cleaned_repository_url = parsed_url.netloc + parsed_url.path\n","                    else:\n","                        cleaned_repository_url = repository_url.lstrip('git+')\n","\n","        # Fetching the abbreviated response\n","        response = requests.get(url, headers=headers)\n","        response.raise_for_status()  # Raise an exception for non-200 status codes\n","        abbreviated_package_data = response.json()\n","\n","        # Extracting dependencies, devDependencies, and deprecated from abbreviated response\n","        last_version_data = list(abbreviated_package_data.get('versions', {}).values())[-1] if abbreviated_package_data.get('versions') else None\n","        dependencies = last_version_data.get('dependencies', {}) if last_version_data else {}\n","        dependencies_name = list(dependencies.keys())\n","        dependencies_count = len(dependencies)\n","\n","        dev_dependencies = last_version_data.get('devDependencies', {}) if last_version_data else {}\n","        dev_dependencies_name = list(dev_dependencies.keys())\n","        dev_dependencies_count = len(dev_dependencies)\n","\n","#         deprecated = last_version_data.get('deprecated', None) if last_version_data else None\n","#         deprecated_status = 'Yes' if deprecated else 'No'\n","        # Extracting deprecated status\n","        if last_version_data:\n","            deprecated = last_version_data.get('deprecated')\n","            if isinstance(deprecated, str):\n","                deprecated = deprecated.strip()  # Remove leading and trailing whitespace\n","                if deprecated == '' or deprecated.lower() == 'false':\n","                    deprecated = None  # Treat empty string or 'false' as not deprecated\n","        else:\n","            deprecated = None\n","\n","        # Set deprecated status\n","        deprecated_status = 'Yes' if deprecated else 'No'\n","\n","        # Fetching download stats\n","        download_stats_last_day = requests.get(f'https://api.npmjs.org/downloads/point/last-day/{package_name}').json()\n","        download_stats_last_week = requests.get(f'https://api.npmjs.org/downloads/range/last-week/{package_name}').json()\n","        download_stats_last_month = requests.get(f'https://api.npmjs.org/downloads/range/last-month/{package_name}').json()\n","        download_stats_last_3_month = requests.get(f'https://api.npmjs.org/downloads/range/2023-11-17:2024-02-16/{package_name}').json()\n","        \n","        # Extracting download stats\n","        last_day_downloads = download_stats_last_day['downloads']\n","        last_week_downloads = sum(day['downloads'] for day in download_stats_last_week['downloads'])\n","        last_month_downloads = sum(day['downloads'] for day in download_stats_last_month['downloads'])\n","        last_3_month_downloads = sum(day['downloads'] for day in download_stats_last_3_month['downloads'])\n","\n","        # Create a row for the DataFrame\n","        data_row = {\n","            'package_name': package_name,\n","            'description': description,\n","            'maintainers_length': maintainers_length,\n","            'readmeFilename_exists': readmeFilename_exists,\n","            'created': created,\n","            'modified': modified,\n","            'latest_version': latest_version,\n","            'latest_version_git_repo': cleaned_repository_url,\n","            'dependencies_name': dependencies_name,\n","            'dependencies_count': dependencies_count,\n","            'dev_dependencies_name': dev_dependencies_name,\n","            'dev_dependencies_count': dev_dependencies_count,\n","            'deprecated': deprecated_status,\n","            'last_day_downloads': last_day_downloads,\n","            'last_week_downloads': last_week_downloads,\n","            'last_month_downloads': last_month_downloads,\n","            'last_3_month_downloads':last_3_month_downloads\n","        }\n","\n","        # Append the row to the list\n","        data_rows.append(data_row)\n","\n","    except Exception as e:\n","        print(f\"Error fetching data for {package_name}: {e}\")\n","        # Create a row with all values as 'NA'\n","        error_row = {key: 'NA' for key in data_row.keys()}\n","        error_row['package_name'] = package_name\n","        data_rows.append(error_row)\n","\n","# Create a DataFrame from the list of rows\n","data = pd.DataFrame(data_rows)\n","\n","# Save the DataFrame to a new CSV file\n","data.to_csv('npm_package_data_final.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### ChatGPT - Trials"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-18T22:18:14.394761Z","iopub.status.busy":"2024-02-18T22:18:14.394197Z","iopub.status.idle":"2024-02-18T22:18:14.401195Z","shell.execute_reply":"2024-02-18T22:18:14.400320Z","shell.execute_reply.started":"2024-02-18T22:18:14.394718Z"},"trusted":true},"outputs":[],"source":["# import requests\n","\n","# # Set your OpenAI API key\n","# OPENAI_API_KEY = ''\n","\n","# # Define the request payload\n","# payload = {\n","#     \"model\": \"gpt-3.5-turbo\",\n","#     \"messages\": [{\"role\": \"user\", \"content\": \"can you convert this url to standard git https url 'ssh://git@github.com/officialHaze/Logger.git'\"}],\n","#     \"temperature\": 0.7\n","# }\n","\n","# # Define headers\n","# headers = {\n","#     \"Content-Type\": \"application/json\",\n","#     \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n","# }\n","\n","# # Define the API endpoint\n","# url = \"https://api.openai.com/v1/chat/completions\"\n","\n","# # Send the request\n","# response = requests.post(url, json=payload, headers=headers)\n","\n","# # Print the response\n","# json_response = response.json()\n","\n","# # Extract assistant content\n","# assistant_content = json_response['choices'][0]['message']['content']\n","\n","# # Print the assistant content\n","# print(\"Assistant's Response:\", assistant_content)\n","\n","\n","# !pip install openai\n","\n","# import os\n","# from openai import OpenAI\n","\n","# client = OpenAI(\n","#     # This is the default and can be omitted\n","#     api_key='',\n","# )\n","\n","# comment = \"'ssh://git@github.com/officialHaze/Logger.git'\"\n","# chat_completion = client.chat.completions.create(\n","#     messages=[\n","#         {\n","#             \"role\": \"user\",\n","#             \"content\": f\" can you convert this url to standard git https url  :\\nC: {comment}\\nA:\",\n","#         }\n","#     ],\n","#     model=\"gpt-3.5-turbo\",\n","# )\n","\n","# print(chat_completion.choices[0].message.content)\n"]},{"cell_type":"markdown","metadata":{},"source":["###"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T00:23:49.222336Z","iopub.status.busy":"2024-02-20T00:23:49.221946Z","iopub.status.idle":"2024-02-20T00:24:05.732494Z","shell.execute_reply":"2024-02-20T00:24:05.731030Z","shell.execute_reply.started":"2024-02-20T00:23:49.222305Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pydriller\n","  Obtaining dependency information for pydriller from https://files.pythonhosted.org/packages/85/5e/d7f5e3fc06c4caa693d785b1372de42bd7f31aab5e3b1c383f9c4f5ce9ae/PyDriller-2.6-py3-none-any.whl.metadata\n","  Downloading PyDriller-2.6-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: gitpython in /opt/conda/lib/python3.10/site-packages (from pydriller) (3.1.32)\n","Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from pydriller) (2023.3)\n","Collecting types-pytz (from pydriller)\n","  Obtaining dependency information for types-pytz from https://files.pythonhosted.org/packages/fc/6c/42a83a27bef09295b8d26866d690fd50d53b7fa27a961779daf0bb8f3b0c/types_pytz-2024.1.0.20240203-py3-none-any.whl.metadata\n","  Downloading types_pytz-2024.1.0.20240203-py3-none-any.whl.metadata (1.5 kB)\n","Collecting lizard (from pydriller)\n","  Downloading lizard-1.17.10-py2.py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython->pydriller) (4.0.10)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->pydriller) (5.0.0)\n","Downloading PyDriller-2.6-py3-none-any.whl (33 kB)\n","Downloading types_pytz-2024.1.0.20240203-py3-none-any.whl (5.1 kB)\n","Installing collected packages: lizard, types-pytz, pydriller\n","Successfully installed lizard-1.17.10 pydriller-2.6 types-pytz-2024.1.0.20240203\n"]}],"source":["!pip install pydriller"]},{"cell_type":"markdown","metadata":{},"source":["### Pydriller Final Working"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T01:12:23.850374Z","iopub.status.busy":"2024-02-20T01:12:23.849951Z","iopub.status.idle":"2024-02-20T01:13:43.361946Z","shell.execute_reply":"2024-02-20T01:13:43.360231Z","shell.execute_reply.started":"2024-02-20T01:12:23.850339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Analyzing repository: https://github.com/baneuna/gulp-task-collection\n","Error analyzing repository https://github.com/baneuna/gulp-task-collection: 404 Client Error: Not Found for url: https://github.com/baneuna/gulp-task-collection\n","Analyzing repository: https://github.com/bbanez/basic-auth-file-server\n","Error analyzing repository https://github.com/bbanez/basic-auth-file-server: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=022f2be84d8907f2fe9c92f4c678c4e4939e80b7 2751bee282204e8e9b11cae215b915a7629687d1 --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/browser-storage\n","Analyzing repository: https://github.com/Bbanez/child_process\n","Error analyzing repository https://github.com/Bbanez/child_process: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=9d7377b5ac4232d8f9a4313a40a2a4e70377cad6 c61f7419050c4c1083f46bd8040315950d33c4ee --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/docker\n","Error analyzing repository https://github.com/Bbanez/docker: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=78ad009e4f53b0de7e920151dce1a81f4829b0f0 78d3a736a5dbf91de3b9276aa4be123608dfed28 --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/fs\n","Error analyzing repository https://github.com/Bbanez/fs: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=c236df7caecaef1bdcb14b7e260919cce60ad122 25b2781776a1d97db4cedc84464a20d239cc2cee --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/graphql-client\n","Error analyzing repository https://github.com/Bbanez/graphql-client: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=fd8be66e6df67bdd9607d46552a52f5f9447a7fe 7da7c23fff90ef9e276b25b5c7044568de1a9ae4 --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/bbanez/mem-store\n","Error analyzing repository https://github.com/bbanez/mem-store: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=193481bbefb553608037f9898c96c9d05f636c9e 1629d12c139ff57ecbfb61b9fb88a4172e126111 --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/application-express\n","Error analyzing repository https://github.com/Bbanez/application-express: 404 Client Error: Not Found for url: https://github.com/Bbanez/application-express\n","Analyzing repository: https://github.com/Bbanez/npm-tool\n","Error analyzing repository https://github.com/Bbanez/npm-tool: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=4c626c83689075e067f46f425816ea8b6ecdbb00 8a81b0ff2bae0de8a924113380bb3c10cc11ba7d --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/object-utility\n","Error analyzing repository https://github.com/Bbanez/object-utility: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=ca901274e9cfa5dbb90a19d148b12c5122ff9e75 e4b32f8bbaafe5cccc9ab40b5048da283b1dbbb5 --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/queue\n","Error analyzing repository https://github.com/Bbanez/queue: Cmd('git') failed due to: exit code(129)\n","  cmdline: git rev-list --ancestry-path=62e9e86bacd9fcbadda80dfe004de8ed4598495a 017d26f704d917bcb9222c6db81f6e0a278c29b3 --\n","  stderr: 'usage: git rev-list [OPTION] <commit-id>... [ -- paths... ]\n","  limiting output:\n","    --max-count=<n>\n","    --max-age=<epoch>\n","    --min-age=<epoch>\n","    --sparse\n","    --no-merges\n","    --min-parents=<n>\n","    --no-min-parents\n","    --max-parents=<n>\n","    --no-max-parents\n","    --remove-empty\n","    --all\n","    --branches\n","    --tags\n","    --remotes\n","    --stdin\n","    --quiet\n","  ordering output:\n","    --topo-order\n","    --date-order\n","    --reverse\n","  formatting output:\n","    --parents\n","    --children\n","    --objects | --objects-edge\n","    --unpacked\n","    --header | --pretty\n","    --[no-]object-names\n","    --abbrev=<n> | --no-abbrev\n","    --abbrev-commit\n","    --left-right\n","    --count\n","  special purpose:\n","    --bisect\n","    --bisect-vars\n","    --bisect-all\n","'\n","Analyzing repository: https://github.com/Bbanez/search\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 285\u001b[0m\n\u001b[1;32m    282\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterows(data)\n\u001b[1;32m    284\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/pydriller-test-2/npm_package_data_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 285\u001b[0m \u001b[43mfetch_and_analyze_repositories\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgit_stats.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# added lines & deleted lines -> done\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# pr freq -> done \u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# release history -> done\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# LOC ->done\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# readme updated -> done\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 249\u001b[0m, in \u001b[0;36mfetch_and_analyze_repositories\u001b[0;34m(input_csv, output_csv)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_url \u001b[38;5;129;01mand\u001b[39;00m repo_url \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m unique_repos:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing repository: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 249\u001b[0m     total_commits, avg_commits_per_day, last_commit_date, unique_contributors, forks, stars, open_prs_count, merged_prs_count, open_issues, resolved_issues, pr_frequency, lines_of_codes, total_added, total_deleted, lines_added_one_and_half_year, lines_deleted_one_and_half_year, readme_updated \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_commits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgit_repo\u001b[39m\u001b[38;5;124m'\u001b[39m: repo_url,\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_commits\u001b[39m\u001b[38;5;124m'\u001b[39m: total_commits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadme_updated\u001b[39m\u001b[38;5;124m'\u001b[39m: readme_updated\n\u001b[1;32m    271\u001b[0m         })\n","Cell \u001b[0;32mIn[12], line 206\u001b[0m, in \u001b[0;36manalyze_repository\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    204\u001b[0m forks, stars \u001b[38;5;241m=\u001b[39m fetch_forks_stars(url)\n\u001b[1;32m    205\u001b[0m open_issues \u001b[38;5;241m=\u001b[39m issues_pending(url)\n\u001b[0;32m--> 206\u001b[0m resolved_issues \u001b[38;5;241m=\u001b[39m \u001b[43missues_resolved\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m total_commits, avg_commits_per_day, last_commit_date, unique_contributors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    209\u001b[0m repo \u001b[38;5;241m=\u001b[39m Repository(url)\n","Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36missues_resolved\u001b[0;34m(git_repo_url)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missues_resolved\u001b[39m(git_repo_url):\n\u001b[0;32m---> 52\u001b[0m     result\u001b[38;5;241m=\u001b[39m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgit_repo_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     html \u001b[38;5;241m=\u001b[39m lx\u001b[38;5;241m.\u001b[39mfromstring(result\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     54\u001b[0m     issues_tab \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//a[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missues-tab\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n","File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import csv\n","from pydriller import Repository\n","from pydriller.metrics.process.lines_count import LinesCount\n","from collections import defaultdict\n","import requests\n","import lxml.html as lx\n","from datetime import datetime, timedelta\n","import time\n","import pandas as pd\n","import requests\n","from urllib.parse import urlparse\n","import random\n","\n","def pull_request_frequency(repo_link):\n","    username, repo_name = repo_link.split('/')[-2:]\n","    \n","    open_prs_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=open'\n","    open_prs_response = requests.get(open_prs_url)\n","    open_prs_count = len(open_prs_response.json())\n","    \n","    merged_prs_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=closed&sort=updated&direction=desc'\n","    merged_prs_response = requests.get(merged_prs_url)\n","    merged_prs_count = len(merged_prs_response.json())\n","    \n","    return open_prs_count, merged_prs_count\n","\n","def fetch_forks_stars(url):\n","    response = requests.get(url)\n","    response.raise_for_status()\n","    html = lx.fromstring(response.text)\n","    fork_element = html.xpath('//*[@id=\"repo-network-counter\"]')[0]\n","    star_element = html.xpath('//*[@id=\"repo-stars-counter-star\"]')[0]\n","    try:\n","        forks = int(fork_element.text)\n","    except:\n","        if(fork_element.text and fork_element.text[-1] == 'k'):\n","            forks = int(fork_element.text[:-1])*1000\n","        else:\n","            forks = fork_element.text\n","    \n","    try:\n","        stars = int(star_element.text)\n","    except:\n","        if(star_element.text and star_element.text[-1] == 'k'):\n","            stars = int(star_element.text[:-1])*1000\n","        else:\n","            stars = star_element.text\n","            \n","    return forks, stars\n","\n","def issues_pending(git_repo_url):\n","    result=requests.get(git_repo_url)\n","    html = lx.fromstring(result.text)\n","    issues_tab = html.xpath('//a[@id=\"issues-tab\"]')\n","    if issues_tab:\n","        issues_url = git_repo_url + \"/issues\"\n","        result=requests.get(issues_url)\n","        html = lx.fromstring(result.text)\n","        open_issues = html.xpath('//a[@data-ga-click=\"Issues, Table state, Open\"]')\n","        if open_issues:\n","            open_issues_content = open_issues[0].text_content()\n","            open_issues_count = int(open_issues_content.split()[0].replace(',', ''))\n","            return open_issues_count\n","        else:\n","            return 0\n","    else:\n","        return 0\n","    \n","def issues_resolved(git_repo_url):\n","    result=requests.get(git_repo_url)\n","    html = lx.fromstring(result.text)\n","    issues_tab = html.xpath('//a[@id=\"issues-tab\"]')\n","    if issues_tab:\n","        issues_url = git_repo_url + \"/issues\"\n","        result=requests.get(issues_url)\n","        html = lx.fromstring(result.text)\n","        closed_issues = html.xpath('//a[@data-ga-click=\"Issues, Table state, Closed\"]')\n","        if closed_issues:\n","            closed_issues_content = closed_issues[0].text_content()\n","            closed_issues_count = int(closed_issues_content.split()[0].replace(',', ''))\n","            return closed_issues_count\n","        else:\n","            return 0\n","    else:\n","        return 0\n","    \n","# def make_github_request(url):\n","#     while True:\n","#         token = \"\"  # Add your GitHub personal access token here\n","#         headers = {'Authorization': f'Bearer {token}'}\n","#         response = requests.get(url, headers=headers)\n","#         if response.status_code == 200:\n","#             return response\n","#         elif response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n","#             sleep_time = get_rate_limit_reset_time(response) + 5  # Adding 5 seconds to be safe\n","#             print(f\"Rate limit exceeded. Waiting for {sleep_time} seconds before retrying.\")\n","#             time.sleep(sleep_time)\n","#         else:\n","#             response.raise_for_status()\n","\n","# def get_rate_limit_reset_time(response):\n","#     rate_limit_reset = int(response.headers.get('X-RateLimit-Reset', 0))\n","#     return max(rate_limit_reset - time.time(), 0)\n","\n","def pr_freq(repo_link, total_days):\n","    username, repo_name = repo_link.split('/')[-2:]\n","\n","    pr_url = f'https://api.github.com/repos/{username}/{repo_name}/pulls?state=all'\n","    response = requests.get(pr_url)\n","    all_prs = response.json()\n","\n","    total_pull_requests = len(all_prs)\n","    pull_request_frequency = total_pull_requests / total_days\n","\n","    return pull_request_frequency\n","\n","def release_frequency(github_repo_url):\n","    release_dates = []\n","    for commit in Repository(github_repo_url, only_releases=True).traverse_commits():\n","        release_dates.append(commit.committer_date)\n","    \n","    if(len(release_dates)>1):\n","        # Calculate the difference between the last and first release dates\n","        difference = release_dates[-1] - release_dates[0]\n","        \n","        # Get the number of days from the timedelta object\n","        number_of_days = difference.days\n","        number_of_releases = len(release_dates)\n","    \n","        release_frequency_days = number_of_releases / number_of_days\n","    \n","        # Convert release frequency to months and years\n","        release_frequency_months = number_of_releases / (number_of_days / 30)  # Assuming 30 days in a month\n","        release_frequency_years = number_of_releases / (number_of_days / 365)  # Assuming 365 days in a year\n","\n","    else:\n","        return 0,0,0\n","\n","    return release_frequency_days, release_frequency_months, release_frequency_years\n","\n","\n","def get_loc(github_repo_url):\n","    total_added = 0\n","    total_deleted = 0\n","    \n","    commit_dates = []\n","    for commit in Repository(github_repo_url).traverse_commits():\n","        commit_dates.append(commit.committer_date)\n","        \n","    if(len(commit_dates)!=0):\n","        first_commit_date = commit_dates[0]\n","        last_commit_date = commit_dates[-1]\n","    \n","        # Initialize LinesCount\n","        lines_count = LinesCount(path_to_repo=github_repo_url,since=first_commit_date,to=last_commit_date)\n","        \n","        total_added = lines_count.count_added()\n","        total_deleted = lines_count.count_removed()\n","    else:\n","        return 0\n","        \n","    LOC = {}\n","    for key in total_added:\n","        if key in total_deleted:\n","            LOC[key] = total_added[key] - total_deleted[key]\n","        else:\n","            LOC[key] = total_added[key]\n","\n","    Filtered_LOC = {}\n","    \n","    for file in LOC:\n","        if(file is not None and len(file)>3 and file[-3:] == \".js\"):\n","            if(\"test\" not in file and \"tests\" not in file and \".spec.js\" not in file and \".test.js\" not in file and \".spec.ts\" not in file and \".test.ts\" not in file and \"spec\" not in file):\n","                if(\"node_modules\\\\\" not in file and \"public\\\\\" not in file and \"build\\\\\" not in file and \"test\\\\\" not in file):\n","                    Filtered_LOC[file] = LOC[file]\n","    return sum(Filtered_LOC.values())\n","\n","\n","def get_total_lines_added_deleted(github_repo_url):\n","    total_added = 0\n","    total_deleted = 0\n","    \n","    commit_dates = []\n","    for commit in Repository(github_repo_url).traverse_commits():\n","        commit_dates.append(commit.committer_date)\n","        \n","    if(len(commit_dates)!=0):\n","        first_commit_date = commit_dates[0]\n","        last_commit_date = commit_dates[-1]\n","    \n","        # Initialize LinesCount\n","        lines_count = LinesCount(path_to_repo=github_repo_url,since=first_commit_date,to=last_commit_date)\n","        \n","        total_added = lines_count.count_added()\n","    \n","        total_deleted = lines_count.count_removed()\n","\n","    else:\n","        return 0, 0\n","\n","    return sum(total_added.values()), sum(total_deleted.values())\n","\n","\n","def get_lines_added_deleted_last_one_and_half_years(github_repo_url):\n","    total_added = 0\n","    total_deleted = 0\n","    current_date = datetime.now()\n","    one_and_a_half_years_ago = current_date - timedelta(days=547)  # Assuming 1 year = 365 days so 547 for 1.5 years\n","    year = one_and_a_half_years_ago.year\n","    month = one_and_a_half_years_ago.month\n","    day = one_and_a_half_years_ago.day\n","    one_and_a_half_years_ago_date = datetime(year, month, day)\n","    lines_count = LinesCount(path_to_repo=github_repo_url, since=one_and_a_half_years_ago_date, to=current_date)\n","    total_added = lines_count.count_added()\n","    total_deleted = lines_count.count_removed()\n","    return sum(total_added.values()), sum(total_deleted.values())\n","\n","def is_readme_updated_in_last_one_and_half_years(github_repo_url):\n","    total_added = 0\n","    total_deleted = 0\n","\n","    # Get the current date\n","    current_date = datetime.now()\n","    \n","    # Calculate the date 1.5 years ago\n","    one_and_a_half_years_ago = current_date - timedelta(days=547)  # Assuming 1 year = 365 days\n","    \n","    # Extract year, month, and day components\n","    year = one_and_a_half_years_ago.year\n","    month = one_and_a_half_years_ago.month\n","    day = one_and_a_half_years_ago.day\n","    \n","    one_and_a_half_years_ago_date = datetime(year, month, day)\n","    \n","    # Initialize LinesCount\n","    lines_count = LinesCount(path_to_repo=github_repo_url, since=one_and_a_half_years_ago_date, to=current_date)\n","    \n","    total_added = lines_count.count_added()\n","\n","    total_deleted = lines_count.count_removed()\n","    \n","    lowercase_total_added = {key.lower() if key is not None else key: value for key, value in total_added.items()}\n","    lowercase_total_deleted = {key.lower() if key is not None else key: value for key, value in total_deleted.items()}\n","\n","    try:\n","        readme_added = lowercase_total_added['readme.md']\n","        readme_deleted = lowercase_total_deleted['readme.md']\n","        if(readme_added+readme_deleted==0):\n","            readme_updated = False\n","        else:\n","            readme_updated = True\n","    except:\n","        readme_updated = False\n","\n","    return readme_updated\n","\n","\n","def analyze_repository(url):\n","    total_commits = 0\n","    commit_dates = []\n","    contributors = set()\n","\n","    try:\n","        forks, stars = fetch_forks_stars(url)\n","        open_issues = issues_pending(url)\n","        resolved_issues = issues_resolved(url)\n","        total_commits, avg_commits_per_day, last_commit_date, unique_contributors = 0, 0, None, 0\n","\n","        repo = Repository(url)\n","        for commit in repo.traverse_commits():\n","            total_commits += 1\n","            commit_dates.append(commit.committer_date)\n","            contributors.add(commit.author.email)\n","\n","        if total_commits > 0:\n","            first_commit_date = min(commit_dates)\n","            last_commit_date = max(commit_dates)\n","            unique_contributors = len(contributors)\n","            total_days = (last_commit_date - first_commit_date).days + 1\n","            avg_commits_per_day = total_commits / total_days if total_days > 0 else 0\n","\n","        open_prs_count, merged_prs_count = pull_request_frequency(url)\n","        pr_frequency = pr_freq(url, total_days)\n","        pr_frequency = round(pr_frequency, 2)\n","        version_release_frequency_days, version_release_frequency_months, version_release_frequency_years = release_frequency(url)\n","        lines_of_codes = get_loc(url)\n","#         lines_of_codes = None\n","        total_added, total_deleted = get_total_lines_added_deleted(url)\n","        lines_added_one_and_half_year,lines_deleted_one_and_half_year = get_lines_added_deleted_last_one_and_half_years(url)\n","        readme_updated = is_readme_updated_in_last_one_and_half_years(url)\n","        \n","\n","        return total_commits, avg_commits_per_day, last_commit_date, unique_contributors, forks, stars, open_prs_count, merged_prs_count, open_issues, resolved_issues, pr_frequency, version_release_frequency_days, version_release_frequency_months, version_release_frequency_years, lines_of_codes, total_added, total_deleted, lines_added_one_and_half_year, lines_deleted_one_and_half_year, readme_updated\n","    except Exception as e:\n","        print(f\"Error analyzing repository {url}: {e}\")\n","        return None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n","\n","def fetch_and_analyze_repositories(git_repos, output_csv):\n","    unique_repos = set()\n","    data = []\n","        \n","    for repo_url in git_repos:\n","#             repo_url = row.get('latest_version_git_repo')\n","        if repo_url and repo_url not in unique_repos:\n","            print(f\"Analyzing repository: {repo_url}\")\n","            total_commits, avg_commits_per_day, last_commit_date, unique_contributors, forks, stars, open_prs_count, merged_prs_count, open_issues, resolved_issues, pr_frequency, version_release_frequency_days, version_release_frequency_months, version_release_frequency_years, lines_of_codes, total_added, total_deleted, lines_added_one_and_half_year, lines_deleted_one_and_half_year, readme_updated = analyze_repository(repo_url)\n","            if total_commits is not None:\n","                data.append({\n","                    'git_repo': repo_url,\n","                    'total_commits': total_commits,\n","                    'avg_commits_per_day': avg_commits_per_day,\n","                    'last_commit_date': last_commit_date,\n","                    'unique_contributors': unique_contributors,\n","                    'forks': forks,\n","                    'stars': stars,\n","                    'open_PRs': open_prs_count,\n","                    'merged_PRs': merged_prs_count,\n","                    'open_issues': open_issues,\n","                    'resolved_issues': resolved_issues,\n","                    'pr_frequency': pr_frequency,\n","                    'version_release_frequency_days': version_release_frequency_days, \n","                    'version_release_frequency_months': version_release_frequency_months, \n","                    'version_release_frequency_years': version_release_frequency_years,\n","                    'LOC' : lines_of_codes,\n","                    'total_lines_added': total_added,\n","                    'total_lines_deleted': total_deleted,\n","                    'lines_added_one_and_half_year': lines_added_one_and_half_year,\n","                    'lines_deleted_one_and_half_year': lines_deleted_one_and_half_year,\n","                    'readme_updated': readme_updated\n","                })\n","            unique_repos.add(repo_url)\n","        elif repo_url:\n","            print(f\"Skipping duplicate repository: {repo_url}\")\n","        else:\n","            print(\"Skipping repository: value not present\")\n","\n","    with open(output_csv, 'w', newline='') as file:\n","        fieldnames = ['git_repo', 'total_commits', 'avg_commits_per_day', 'last_commit_date', 'unique_contributors', 'forks', 'stars', 'open_PRs', 'merged_PRs', 'open_issues', 'resolved_issues', 'pr_frequency', 'version_release_frequency_days', 'version_release_frequency_months', 'version_release_frequency_years' 'LOC', 'total_lines_added', 'total_lines_deleted', 'lines_added_one_and_half_year', 'lines_deleted_one_and_half_year', 'readme_updated']\n","        writer = csv.DictWriter(file, fieldnames=fieldnames)\n","        writer.writeheader()\n","        writer.writerows(data)\n","\n","\n","input_file = '/kaggle/input/random-batch03/name_git_repo_nik.csv'\n","with open(input_file, 'r') as file:\n","    reader = csv.DictReader(file)\n","    # Handle potential header row\n","    is_header = True\n","    git_repos = []\n","    for row in reader:\n","        if is_header:\n","            is_header = False  # Skip the header row if present\n","            continue\n","        git_repos.append(row['latest_version_git_repo'])  # Assuming git_repos are in the first column\n","\n","    git_repos = git_repos[2000:3000]\n","    print(len(git_repos))\n","        \n","fetch_and_analyze_repositories(git_repos, 'git_stats_batch3.csv')\n","\n","# SLOC -> Nik\n","# complexity -> Nik"]},{"cell_type":"markdown","metadata":{},"source":["### Vulnerability Main run on Local"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import csv\n","import requests\n","import urllib\n","import time\n","from selenium import webdriver\n","from bs4 import BeautifulSoup\n","\n","packages = ['--ignore-scripts', '--ignore-workspace-root-check', '--legacy-peer-deps', '--lockfile-only', '--no-audit', '--only-prod', '--prefer-offline', '--save-bundle', '--wtfnpmexmple--', '-8080', '-8888', '-adisagar2003-react-share-on-social', '-blanc-server-project', '-cascade-all', '-czar23', '-d3-ushape', '-ethomson-test-package', '-f0re7tpunk-resume', '-g-bypong', '-g-u-a-r-d-a-john-wick-4-streaming-ita-alta-wae', '-g-u-a-r-d-a-john-wick-4-streaming-ita-wae', '-global-styles-react-app', '-graphql-codegen-client-preset-swc-test', '-gzip-ize', '-harseurl', '-hes-plugion', '-keyboardevent', '-legit-pixel-gun-3d-gems-generator-2023-no-verify', '-lidonghui', '-liuxin', '-llscw-react-cli', '-lorenzovecchi-react-native-esptouch-2-forked', '-mfxp', '-npm_publish2.0', '-pdf-new-method-boom-beach-mobile-new-diamonds-767', '-reaume', '-redux-forjm', '-sonforms', '-temp-electron-manager-somiibo', '-template-1.0.0xuanyi', '-test-bitbucket-branch-manager', '-test-el', '-tlecoeuv', '-tompan-reacttemplate', '-unete-cli-typescript', '-unlimited-minecoins-minecraft-generator-no-human-verification546', '-users-random-messages', '-wlq-mxgrapheditor', '-wulinlong9.10rk', '-xielingxin-stack-javascript', '-ydfwwmkmmkmklkmffwfw', '0', '0-', '0---top', '0--0double', '0-1-project', '0-100', '0-24', '0-3day1', '0-60', '0-9', '0-_-0', '0-dd-trace', '0-dd-trace-first', '0-dd-trace-init', '0-dd-trace-init-first-datadog-enabled', '0-dd-trace-init-first-production', '0-dns', '0-hun_ajax', '0-logger', '0-npm', '0-plume-css', '0-shadowenv', '0-snap-score318', '0-this-is-a-test', '0-zhuangbei-lan-de-zuiqiang-jianshi-danshi-zuzhou-zhuangbei-keai-dehua-neng-suiyi-zhuang-jiujiujiujiu-ge-wo-de-zuzhou-zhuangbei-bu-keneng-zheme-keai-banmuchi-wan-erlingeryilingererliu', '0.', '0.0', '0.0.0.0', '0.0.1', '0.0.168', '0.0.250', '0.0.9-alpha.5', '0.1.unity-settings-daemon', '0.1f', '0.2-ui', '0.2.18', '0.3.20-beta.11', '0.3package', '0.8.18-p11', '0.app1', '0.apppack', '0.cli', '0.css', '0.draw.io', '0.edsql', '0.edsql.build', '0.edsql.electron', '0.edsql.electronapp', '0.edsql.mysql', '0.edsql.pack1', '0.edsql.react', '0.electron.main', '0.electron.preload', '0.extends.draw.io', '0.extends.electron', '0.extends.fc', '0.extends.packapp', '0.extends.react', '0.extends.sql', '0.extends.vscode', '0.extends.wechat', '0.extends.whistle', '0.js', '0.pack', '0.sql', '0.tsx', '0.workspace', '00', '00-components', '00-test', '00-webpack-numberss', '00.demo', '000-webpack', '0000000r34api', '0000lqb', '0000lqb1', '0002npm', '0003-lion-lib', '00050-demon-slayer-kimetsu-no-yaiba-to-the-swordsmith-village-full-movies-online-at-home-release', '00050-shin-kamen-rider-full-movies-online-at-home-release', '000demo', '001', '001-custom-test', '001-mypackage', '001-nodelist', '001-npm', '001-webpack-numbers', '001128jsckgcl', '001_skt']\n","\n","packages = packages[10:35]\n","output_filename = \"package_data.csv\"\n","\n","def get_package_data(package):\n","    retry_attempts = 3  # Number of retry attempts\n","    for _ in range(retry_attempts):\n","        try:\n","            url = 'https://snyk.io/advisor/npm-package/{}'.format(urllib.parse.quote_plus(package))\n","            print(\"pkg:\", package, \"-\", \"GET\", url)\n","            # Set up Selenium WebDriver\n","            options = webdriver.ChromeOptions()\n","            options.add_argument('--headless')\n","            driver = webdriver.Chrome(options=options)\n","            driver.get(url)\n","\n","            # Wait for the page to fully load\n","            time.sleep(1.3)  # Adjust the sleep time as needed\n","\n","            # Extract HTML content after waiting\n","            html_content = driver.page_source\n","            soup = BeautifulSoup(html_content, \"html.parser\")\n","\n","            container_elements = soup.find_all('div', class_='container')\n","            values_list = []\n","\n","            for container in container_elements:\n","                values = container.find_all('div', class_='item')\n","                for value in values:\n","                    span = value.find('span')\n","                    if span:\n","                        values_list.append(span.text.strip())\n","            latest_version = license = health_score = security_recommendation = popularity = maintainence = community = gitHub_stars = forks = contributors = last_release = last_commit = readme_file = age = dependencies = versions = install_size = num_of_files = maintainers = TS_typings = critical_vul_count = high_vul_count = medium_vul_count = low_vul_count = None\n","            if values_list:\n","                latest_version = values_list[0].replace('Latest version published', '').strip()\n","                license = values_list[1].replace('License: ', '').strip()\n","\n","            number_div = soup.find('div', class_='number')\n","            if number_div:\n","                health_score = number_div.find('span').text.strip()[:3].rstrip()\n","\n","            ul_element = soup.find('ul', class_='scores')\n","            if ul_element:\n","                spans = ul_element.find_all('span', class_='vue--pill__body')\n","                security_recommendation = spans[0].text.strip()\n","                popularity = spans[1].text.strip()\n","                maintainence = spans[2].text.strip()\n","                community = spans[3].text.strip()\n","\n","            stats_dl = soup.find_all('dl', class_='stats stats--fluid')\n","            span_values = []\n","            for dl in stats_dl:\n","                spans = dl.find_all('span')\n","                span_values += [span.text.strip() for span in spans]\n","            if span_values:\n","                gitHub_stars = span_values[1]\n","                forks = span_values[3]\n","                contributors = span_values[5]\n","                last_release = span_values[-3]\n","                last_commit = span_values[-1]\n","\n","            stats_grid = soup.find_all('dl', class_='stats stats--grid')\n","            stats_grid_values = []\n","            for grid in stats_grid:\n","                grids = grid.find_all('span')\n","                stats_grid_values += [grid1.text.strip() for grid1 in grids]\n","            if stats_grid_values:\n","                readme_file = stats_grid_values[1]\n","                age = stats_grid_values[11]\n","                dependencies = stats_grid_values[13]\n","                versions = stats_grid_values[15]\n","                install_size = stats_grid_values[17]\n","                num_of_files = stats_grid_values[21]\n","                maintainers = stats_grid_values[23]\n","                TS_typings = stats_grid_values[25]\n","\n","            # Find the table with class vue--security-severity-table\n","            table = soup.find('table', class_='vue--security-severity-table')\n","            vulnerabilities_count = []\n","            if table:\n","                # Find the row with class vue--security-severity-table__highlighted\n","                highlighted_row = table.find('tr', class_='vue--security-severity-table__highlighted')\n","                if highlighted_row:\n","                    # Find the ul with class vue--severity within the highlighted row\n","                    ul = highlighted_row.find('ul', class_='vue--severity')\n","                    if ul:\n","                        # Find all li tags within the ul\n","                        li_tags = ul.find_all('li')\n","                        for li in li_tags:\n","                            # Find all div tags with class vue--severity__count within each li\n","                            div_tags = li.find_all('div', class_='vue--severity__count')\n","                            for div in div_tags:\n","                                # Find all span tags within each div and extract their text\n","                                span_tags = div.find_all('span')\n","                                for span in span_tags:\n","                                    vulnerabilities_count.append(span.text.strip())\n","            if vulnerabilities_count:\n","                critical_vul_count = vulnerabilities_count[0]\n","                high_vul_count = vulnerabilities_count[1]\n","                medium_vul_count = vulnerabilities_count[2]\n","                low_vul_count = vulnerabilities_count[3]\n","\n","            driver.quit()\n","\n","            if any(value == 'pending...' or value.strip() == '?' for value in [latest_version, license, health_score]):\n","                    print(\"Retrying for package:\", package)\n","                    continue  # Retry for this package\n","            else:\n","                    return [package, latest_version, license, health_score, security_recommendation, popularity, maintainence, community, gitHub_stars, forks, contributors, last_release, last_commit, readme_file, age, dependencies, versions, install_size, num_of_files, maintainers, TS_typings, critical_vul_count, high_vul_count, medium_vul_count, low_vul_count]\n","\n","        except Exception as e:\n","            print(\"Error occurred while processing package:\", package)\n","            print(e)\n","            return None\n","\n","    print(\"Failed to retrieve data for package after\", retry_attempts, \"attempts:\", package)\n","    return None\n","\n","with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n","    fieldnames = ['Package', 'Latest Version', 'License', 'Health Score', 'Security Recommendation', 'Popularity', 'Maintainence', 'Community', 'GitHub Stars', 'Forks', 'Contributors', 'Last Release', 'Last Commit', 'Readme File', 'Age', 'Dependencies', 'Versions', 'Install Size', '# of Files', 'Maintainers', 'TS Typings', 'critical_vul_count', 'high_vul_count', 'medium_vul_count', 'low_vul_count']\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    for package in packages:\n","        package_data = get_package_data(package)\n","        if package_data:\n","            writer.writerow(dict(zip(fieldnames, package_data)))\n","\n","print(\"Data written to\", output_filename)"]},{"cell_type":"markdown","metadata":{},"source":["### FIletering for parent-package"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T01:36:43.946375Z","iopub.status.busy":"2024-02-20T01:36:43.945924Z","iopub.status.idle":"2024-02-20T01:36:43.952224Z","shell.execute_reply":"2024-02-20T01:36:43.950928Z","shell.execute_reply.started":"2024-02-20T01:36:43.946342Z"},"trusted":true},"outputs":[],"source":["#code"]},{"cell_type":"markdown","metadata":{},"source":["### Try"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T22:54:41.832649Z","iopub.status.busy":"2024-02-19T22:54:41.832167Z","iopub.status.idle":"2024-02-19T22:54:42.297430Z","shell.execute_reply":"2024-02-19T22:54:42.295835Z","shell.execute_reply.started":"2024-02-19T22:54:41.832615Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]},{"data":{"text/plain":["0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import requests\n","def get_rate_limit_reset_time(response):\n","    rate_limit_reset = int(response.headers.get('X-RateLimit-Reset', 0))\n","    return max(rate_limit_reset - time.time(), 0)\n","\n","def release_history(repo_link):\n","    # Get the username and repository name from the link\n","    username, repo_name = repo_link.split('/')[-2:]\n","    \n","    # Get the list of releases for the repository\n","    releases_url = f'https://api.github.com/repos/{username}/{repo_name}/releases'\n","    response = requests.get(releases_url)\n","    releases = response.json()\n","    print(releases)\n","    \n","    if releases:\n","        # Extracting published dates\n","        published_dates = [release['published_at'] for release in releases]\n","        # Sorting dates chronologically\n","        sorted_dates = sorted(published_dates)\n","        # Converting dates from string to datetime format\n","        datetime_dates = [datetime.strptime(date, '%Y-%m-%dT%H:%M:%SZ') for date in sorted_dates]\n","        # Calculate frequency between the first and last release\n","        first_release_date = datetime_dates[0]\n","        last_release_date = datetime_dates[-1]\n","        total_days = (last_release_date - first_release_date).days + 1\n","        release_frequency = len(releases) / total_days if total_days > 0 else 0\n","        return release_frequency\n","    elif response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n","        sleep_time = get_rate_limit_reset_time(response) + 5  # Adding 5 seconds to be safe\n","        print(f\"Rate limit exceeded. Waiting for {sleep_time} seconds before retrying.\")\n","        time.sleep(sleep_time)\n","        return release_history(repo_link)\n","    else:\n","        return 0\n","    \n","\n","release_history('https://github.com/Bbanez/browser-storage')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4289214,"sourceId":7380661,"sourceType":"datasetVersion"},{"datasetId":4461260,"sourceId":7652536,"sourceType":"datasetVersion"},{"datasetId":4461652,"sourceId":7653150,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
